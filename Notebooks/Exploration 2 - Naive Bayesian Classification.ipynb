{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def lex(statement):\n",
    "    words = []\n",
    "    for w in word_tokenize(statement.lower()):\n",
    "        if w not in stopWords:\n",
    "            words.append(w)\n",
    "    return words\n",
    "    return [ w for w in word_tokenize(statement) if w not in stopWords]\n",
    "\n",
    "\n",
    "def parse(lexed_statement,ns=(1,2),skipgrams=True):\n",
    "    out = []\n",
    "    for n in ns:\n",
    "        for i in range(len(lexed_statement)):\n",
    "            out.append(tuple(lexed_statement[i:i+n]))\n",
    "    for index, item in enumerate(lexed_statement):\n",
    "        try:\n",
    "            out.append((item,lexed_statement[index+2]))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return set(tuple(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_text_mapping = {\n",
    "    'pants-fire':0,\n",
    "    'false':0.1,\n",
    "    'barely-true':0.25,\n",
    "    'half-true':0.5,\n",
    "    'mostly-true':0.75,\n",
    "    'true':1\n",
    "}\n",
    "class Statement:\n",
    "    def __init__(self, body, speaker, value,context):\n",
    "        self.body = body\n",
    "        self.speaker = speaker\n",
    "        self.value = truth_text_mapping[value]\n",
    "        self.context = context\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def from_row(row):\n",
    "        return Statement(value=row[1],\n",
    "                         body=row[2],\n",
    "                         speaker=row[4],\n",
    "                         context=row[13])\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        arg_str =  str(', '.join(['='.join([i[0],repr(i[1])]) for i in vars(self).items()]))\n",
    "        return \"Statement({})\".format(arg_str)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Statement(body='Says the Annies List political group supports third-trimester abortions on demand.', value=0.1, speaker='dwayne-bohac', context='a mailer'),\n",
       " Statement(body='When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.', value=0.5, speaker='scott-surovell', context='a floor speech.'),\n",
       " Statement(body='Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"', value=0.75, speaker='barack-obama', context='Denver'),\n",
       " Statement(body='Health care reform legislation is likely to mandate free sex change surgeries.', value=0.1, speaker='blog-posting', context='a news release'),\n",
       " Statement(body='The economic turnaround started at the end of my term.', value=0.5, speaker='charlie-crist', context='an interview on CNN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_liar_data(path):\n",
    "    statements = []\n",
    "    with open(path) as data_file:\n",
    "        reader = csv.reader(data_file, delimiter='\\t', quotechar='\"')\n",
    "        for row in reader:\n",
    "            try:\n",
    "                statements.append(Statement.from_row(row))\n",
    "            except IndexError:\n",
    "                print(row,len(row))\n",
    "    return statements\n",
    "statements = load_liar_data(\"../datasets/LIAR/train.tsv\") \n",
    "# print out some statements to verify by eye.\n",
    "statements[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, statements,lexer=lex,parser=parse):\n",
    "        self.lexer = lexer\n",
    "        self.parser = parser\n",
    "        self.classes = set()\n",
    "        \n",
    "        self.vocabulary = set()\n",
    "\n",
    "        # count number of occurences of each word tuple with each truth value\n",
    "        word_value_counters = dict()\n",
    "        for s in statements:\n",
    "            # construct a set of all word tuples in the dataset\n",
    "            lexed  = lexer(s)\n",
    "            parsed = parser(lexed)\n",
    "            self.vocabulary.update(parsed)\n",
    "            for word_tuple in parsed:\n",
    "                if word_tuple not in word_value_counters:\n",
    "                    word_value_counters[word_tuple] =  defaultdict(lambda :0 )\n",
    "                counter = word_value_counters[word_tuple]\n",
    "                counter.update({s.value:counter[s.value]+1}) \n",
    "        # filter out words that have only too few times\n",
    "        word_value_counters = {k:v for k,v in word_value_counters.items() if sum(v.values()) < 10 }\n",
    "        # convert number of occurrences to frequency\n",
    "        for word,counter in word_value_counters.items():\n",
    "            s = sum(counter.values())\n",
    "            for k,v in counter.items():\n",
    "                counter[k] = v/s\n",
    "        self.word_value_counters = word_value_counters\n",
    "        print(\"Classifier vocab size: {}\".format(len(self.vocabulary)))\n",
    "        \n",
    "    def classify_statement(self, statement):\n",
    "#         print(self.word_value_counters)\n",
    "        value_probabilities = {k:[0] for k in truth_text_mapping.values()}\n",
    "        words = self.parser(self.lexer(statement))\n",
    "#         build the list of coefficients for each value\n",
    "        for w in words:\n",
    "            if w in self.word_value_counters:\n",
    "                for k,v in self.word_value_counters[w].items():\n",
    "                    value_probabilities[k].append(v)\n",
    "        # compute products of coefficients\n",
    "        products = defaultdict(lambda: 0)\n",
    "#         print(value_probabilities)\n",
    "        for k,v in value_probabilities.items():\n",
    "            prod = 1\n",
    "            for x in v:\n",
    "                assert x <= 1\n",
    "                prod *= (x+1)\n",
    "            products[k] = prod\n",
    "        # sort output classes based on how well they match the input\n",
    "        sorted_classes = sorted(products.items(), key=lambda t: t[1], reverse=True)\n",
    "        # return only the best match\n",
    "        return sorted_classes[0]\n",
    "    def score(self, statements):\n",
    "        return sum(abs(self.classify_statement(s)[0] - s.value) <= 0.1 for s in statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier vocab size: 130104\n"
     ]
    }
   ],
   "source": [
    "cls = NaiveBayesClassifier(statements,\n",
    "                           lexer = lambda s: lex(' '.join([s.speaker, s.context, s.body])),\n",
    "                           parser = lambda l: parse(l, ns=[1], skipgrams=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(cls, statements):\n",
    "    \"\"\"A function to verify accuracy\"\"\"\n",
    "    c = 0\n",
    "    for s in statements:\n",
    "        if abs(cls.classify_statement(s)[0] - s.value) <= 0.1:\n",
    "            c += 1\n",
    "        else:\n",
    "            pass\n",
    "#             print(cls.classify_statement(s), s.value)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct=10236, set size=10241, fraction=0.9995117664290597\n"
     ]
    }
   ],
   "source": [
    "n = cls.score(statements)\n",
    "ls = len(statements)\n",
    "print(\"Correct={}, set size={}, fraction={}\".format(n,ls,n/ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct=306, set size=1267, fraction=0.24151539068666142\n"
     ]
    }
   ],
   "source": [
    "some_statements = load_liar_data(\"../datasets/LIAR/test.tsv\")\n",
    "n = cls.score(some_statements)\n",
    "ls = len(some_statements)\n",
    "print(\"Correct={}, set size={}, fraction={}\".format(n,ls,n/ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "~25% accuracy is insatisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
