{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repository for this code: https://github.com/TheCDC/CSC413_Midterm_Project\n",
    "\n",
    "Test out tokenization method(s) and verify by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = [ w for w in words if w not in stopWords]\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to strip unwanted input i.e. stopwords, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lex(statement):\n",
    "    words = []\n",
    "    for w in word_tokenize(statement.lower()):\n",
    "        if w not in stopWords:\n",
    "            words.append(w)\n",
    "    return words\n",
    "    return [ w for w in word_tokenize(statement) if w not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(lexed_statement,ns=(1,2),skipgrams=True):\n",
    "    out = []\n",
    "    for n in ns:\n",
    "        for i in range(len(lexed_statement)):\n",
    "            out.append(tuple(lexed_statement[i:i+n]))\n",
    "    for index, item in enumerate(lexed_statement):\n",
    "        try:\n",
    "            out.append((item,lexed_statement[index+2]))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return set(tuple(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('.',),\n",
       " ('.', 'play'),\n",
       " ('.', 'work'),\n",
       " ('boy',),\n",
       " ('boy', '.'),\n",
       " ('boy', 'work'),\n",
       " ('dull',),\n",
       " ('dull', '.'),\n",
       " ('dull', 'boy'),\n",
       " ('jack',),\n",
       " ('jack', 'boy'),\n",
       " ('jack', 'dull'),\n",
       " ('makes',),\n",
       " ('makes', 'dull'),\n",
       " ('makes', 'jack'),\n",
       " ('play',),\n",
       " ('play', 'jack'),\n",
       " ('play', 'makes'),\n",
       " ('work',),\n",
       " ('work', 'makes'),\n",
       " ('work', 'play')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexed  = lex(data)\n",
    "parse(lexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexed_and_parsed(statement):\n",
    "    return parse(lex(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a struct for a statement and also a  mapping from the ambiguous truth values to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth_text_mapping = {\n",
    "    'pants-fire':0,\n",
    "    'false':0.1,\n",
    "    'barely-true':0.25,\n",
    "    'half-true':0.5,\n",
    "    'mostly-true':0.75,\n",
    "    'true':1\n",
    "}\n",
    "class Statement:\n",
    "    def __init__(self, body, speaker, value):\n",
    "        self.body = body\n",
    "        self.speaker = speaker\n",
    "        self.value = truth_text_mapping[value]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def from_row(row):\n",
    "        return Statement(value=row[1], body=row[2], speaker=row[4])\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        arg_str =  str(', '.join(['='.join([i[0],repr(i[1])]) for i in vars(self).items()]))\n",
    "        return \"Statement({})\".format(arg_str)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convenient method for loading the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Statement(value=0.1, speaker='dwayne-bohac', body='Says the Annies List political group supports third-trimester abortions on demand.'),\n",
       " Statement(value=0.5, speaker='scott-surovell', body='When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.'),\n",
       " Statement(value=0.75, speaker='barack-obama', body='Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"'),\n",
       " Statement(value=0.1, speaker='blog-posting', body='Health care reform legislation is likely to mandate free sex change surgeries.'),\n",
       " Statement(value=0.5, speaker='charlie-crist', body='The economic turnaround started at the end of my term.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_liar_data(path):\n",
    "    statements = []\n",
    "    with open(path) as data_file:\n",
    "        reader = csv.reader(data_file, delimiter='\\t', quotechar='\"')\n",
    "        for row in reader:\n",
    "            try:\n",
    "                statements.append(Statement.from_row(row))\n",
    "            except IndexError:\n",
    "                print(row,len(row))\n",
    "    return statements\n",
    "statements = load_liar_data(\"../datasets/LIAR/train.tsv\") \n",
    "# print out some statements to verify by eye.\n",
    "statements[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171406"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for s in statements:\n",
    "    vocabulary.update(lexed_and_parsed(s.body))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times a given word tuple is associated with each truth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_value_counters = dict()\n",
    "for s in statements:\n",
    "    for word_tuple in lexed_and_parsed(s.body):\n",
    "        if word_tuple not in word_value_counters:\n",
    "            word_value_counters[word_tuple] =  defaultdict(lambda :0 )\n",
    "        counter = word_value_counters[word_tuple]\n",
    "        counter.update({s.value:counter[s.value]+1}) \n",
    "word_value_counters = {k:v for k,v in word_value_counters.items() if sum(v.values()) > 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect that output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 804,\n",
       "               0.1: 1907,\n",
       "               0.25: 1609,\n",
       "               0.5: 2040,\n",
       "               0.75: 1916,\n",
       "               1: 1630})),\n",
       " ((',',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 266, 0.1: 566, 0.25: 601, 0.5: 803, 0.75: 728, 1: 572})),\n",
       " (('says',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 229, 0.1: 468, 0.25: 446, 0.5: 487, 0.75: 401, 1: 317})),\n",
       " (('$',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 61, 0.1: 182, 0.25: 192, 0.5: 258, 0.75: 229, 1: 149})),\n",
       " (('percent',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 40, 0.1: 134, 0.25: 133, 0.5: 228, 0.75: 257, 1: 192})),\n",
       " (('(',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 62, 0.1: 166, 0.25: 135, 0.5: 187, 0.75: 177, 1: 140})),\n",
       " ((')',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 62, 0.1: 166, 0.25: 135, 0.5: 187, 0.75: 178, 1: 140})),\n",
       " (('obama',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 105, 0.1: 160, 0.25: 134, 0.5: 153, 0.75: 97, 1: 85})),\n",
       " (('state',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 39, 0.1: 145, 0.25: 107, 0.5: 160, 0.75: 145, 1: 145})),\n",
       " (('president',),\n",
       "  defaultdict(<function __main__.<lambda>>,\n",
       "              {0: 62, 0.1: 132, 0.25: 100, 0.5: 111, 0.75: 76, 1: 71}))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(word_value_counters.items(), reverse=True, key=lambda i: (max(i[1].values()))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert counts to frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('determined',), defaultdict(<function __main__.<lambda>>, {0: 1.0})),\n",
       " (('secretary', 'paying'),\n",
       "  defaultdict(<function __main__.<lambda>>, {1: 1.0})),\n",
       " (('oil', 'hook'), defaultdict(<function __main__.<lambda>>, {0.5: 1.0})),\n",
       " (('scott', 'governor'),\n",
       "  defaultdict(<function __main__.<lambda>>, {0.1: 1.0})),\n",
       " (('businesses', ','), defaultdict(<function __main__.<lambda>>, {0.1: 1.0})),\n",
       " (('sen.', 'bernie'), defaultdict(<function __main__.<lambda>>, {0.5: 1.0})),\n",
       " (('voted', 'house'), defaultdict(<function __main__.<lambda>>, {0.75: 1.0})),\n",
       " (('harmed', '.'), defaultdict(<function __main__.<lambda>>, {0.5: 1.0})),\n",
       " (('reproductive', 'health'),\n",
       "  defaultdict(<function __main__.<lambda>>, {1: 1.0})),\n",
       " (('2008', 'democratic'),\n",
       "  defaultdict(<function __main__.<lambda>>, {0.5: 1.0})),\n",
       " (('states', 'business'),\n",
       "  defaultdict(<function __main__.<lambda>>, {0.5: 1.0})),\n",
       " (('care', 'illegal'), defaultdict(<function __main__.<lambda>>, {0.1: 1.0})),\n",
       " (('people', 'opposed'),\n",
       "  defaultdict(<function __main__.<lambda>>, {0.75: 1.0})),\n",
       " (('doubled', 'year'), defaultdict(<function __main__.<lambda>>, {0.25: 1.0})),\n",
       " (('says', 'director'), defaultdict(<function __main__.<lambda>>, {0.1: 1.0})),\n",
       " (('headquarters', '``'), defaultdict(<function __main__.<lambda>>, {0: 1.0})),\n",
       " (('support', 'democrats'),\n",
       "  defaultdict(<function __main__.<lambda>>, {0.25: 1.0})),\n",
       " (('single', 'year'), defaultdict(<function __main__.<lambda>>, {1: 1.0})),\n",
       " (('inches', 'longer'), defaultdict(<function __main__.<lambda>>, {0: 1.0})),\n",
       " (('says', 'michele'), defaultdict(<function __main__.<lambda>>, {0: 1.0}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word,counter in word_value_counters.items():\n",
    "    s = sum(counter.values())\n",
    "    for k,v in counter.items():\n",
    "        counter[k] = v/s\n",
    "list(sorted(word_value_counters.items(), reverse=True, key=lambda i: (max(i[1].values()))))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class to encapsulate the previously implemented functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, statements):\n",
    "        self.classes = set()\n",
    "        # construct a set of all word tuples in the dataset\n",
    "        vocabulary = set()\n",
    "        for s in statements:\n",
    "            lexed  = lex(s.body)\n",
    "            parsed = parse(lexed, ns=[1,2,3])\n",
    "            vocabulary.update(parsed)\n",
    "        self.vocabulary = vocabulary\n",
    "        # count number of occurences of each word tuple with each truth value\n",
    "        word_value_counters = dict()\n",
    "        for s in statements:\n",
    "            for word_tuple in lexed_and_parsed(s.body):\n",
    "                if word_tuple not in word_value_counters:\n",
    "                    word_value_counters[word_tuple] =  defaultdict(lambda :0 )\n",
    "                counter = word_value_counters[word_tuple]\n",
    "                counter.update({s.value:counter[s.value]+1}) \n",
    "        # filter out words that have only appear once\n",
    "        word_value_counters = {k:v for k,v in word_value_counters.items() if sum(v.values()) < 2 }\n",
    "        # convert number of occurrences to frequency\n",
    "        for word,counter in word_value_counters.items():\n",
    "            s = sum(counter.values())\n",
    "            for k,v in counter.items():\n",
    "                counter[k] = v/s\n",
    "        self.word_value_counters = word_value_counters\n",
    "        print(\"Classifier vocab size: {}\".format(len(self.vocabulary)))\n",
    "        \n",
    "    def classify_statement(self, statement):\n",
    "#         print(self.word_value_counters)\n",
    "        value_probabilities = {k:[0] for k in truth_text_mapping.values()}\n",
    "        words = lexed_and_parsed(statement.body)\n",
    "#         build the list of coefficients for each value\n",
    "        for w in words:\n",
    "            if w in self.word_value_counters:\n",
    "                assert isinstance(self.word_value_counters[w],defaultdict)\n",
    "                for k,v in self.word_value_counters[w].items():\n",
    "#                     assert isinstance(k,float)\n",
    "#                     assert isinstance(v,float)\n",
    "                    value_probabilities[k].append(v)\n",
    "        # compute products of coefficients\n",
    "        products = defaultdict(lambda: 0)\n",
    "#         print(value_probabilities)\n",
    "        for k,v in value_probabilities.items():\n",
    "            prod = 1\n",
    "            for x in v:\n",
    "                prod *= (x+1)\n",
    "            products[k] = prod\n",
    "        # sort output classes based on how well they match the input\n",
    "        sorted_classes = sorted(products.items(), key=lambda t: t[1], reverse=True)\n",
    "        # return only the best match\n",
    "        return sorted_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier vocab size: 275987\n"
     ]
    }
   ],
   "source": [
    "cls = NaiveBayesClassifier(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement(value=1, speaker='robin-vos', body='The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.') (1, 1048576.0)\n"
     ]
    }
   ],
   "source": [
    "s = statements[5]\n",
    "print(s,cls.classify_statement(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(cls, statements):\n",
    "    \"\"\"A function to verify that the classifier does not error on a set of input\"\"\"\n",
    "    c = 0\n",
    "    for s in statements:\n",
    "        c += cls.classify_statement(s)[0] == s.value\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct=10162, set size=10241, fraction=0.9922859095791426\n"
     ]
    }
   ],
   "source": [
    "n = score(cls, statements)\n",
    "print(\"Correct={}, set size={}, fraction={}\".format(n,len(statements),n/len(statements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct=270, set size=1267, fraction=0.21310181531176006\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = load_liar_data(\"../datasets/LIAR/test.tsv\")\n",
    "n = score(cls, testing_dataset)\n",
    "ls = len(testing_dataset)\n",
    "print(\"Correct={}, set size={}, fraction={}\".format(n,ls,n/ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct=284, set size=1284, fraction=0.22118380062305296\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = load_liar_data(\"../datasets/LIAR/valid.tsv\")\n",
    "n = score(cls, testing_dataset)\n",
    "ls = len(testing_dataset)\n",
    "print(\"Correct={}, set size={}, fraction={}\".format(n,ls,n/ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
